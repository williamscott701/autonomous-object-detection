{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unet loaded successfully\n"
     ]
    }
   ],
   "source": [
    "from imports import *\n",
    "from datasets.idd import *\n",
    "from datasets.bdd import *\n",
    "from detection.unet import *\n",
    "from collections import OrderedDict\n",
    "from torch_cluster import nearest\n",
    "from fastprogress import master_bar, progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=8\n",
    "num_epochs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69863/69863 [00:02<00:00, 25953.05it/s]\n"
     ]
    }
   ],
   "source": [
    "path = '/home/jupyter/autonue/data'\n",
    "root_img_path = os.path.join(path,'bdd100k','images','100k')\n",
    "root_anno_path = os.path.join(path,'bdd100k','labels')\n",
    "\n",
    "train_img_path = root_img_path+'/train/'\n",
    "val_img_path = root_img_path+'/val/'\n",
    "\n",
    "train_anno_json_path = root_anno_path+'/bdd100k_labels_images_train.json'\n",
    "val_anno_json_path = root_anno_path+'/bdd100k_labels_images_val.json'\n",
    "\n",
    "print(\"Loading files\")\n",
    "\n",
    "with open(\"datalists/bdd100k_train_images_path.txt\", \"rb\") as fp:\n",
    "    train_img_path_list = pickle.load(fp)\n",
    "with open(\"datalists/bdd100k_val_images_path.txt\", \"rb\") as fp:\n",
    "    val_img_path_list = pickle.load(fp)\n",
    "\n",
    "src_dataset = dset = BDD(train_img_path_list,train_anno_json_path,get_transform(train=True))\n",
    "src_dl =  torch.utils.data.DataLoader(src_dataset, batch_size=batch_size, shuffle=True, num_workers=4,collate_fn=utils.collate_fn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datalists/idd_images_path_list.txt\", \"rb\") as fp:\n",
    "    non_hq_img_paths = pickle.load(fp)\n",
    "with open(\"datalists/idd_anno_path_list.txt\", \"rb\") as fp:\n",
    "    non_hq_anno_paths = pickle.load(fp)\n",
    "\n",
    "with open(\"datalists/idd_hq_images_path_list.txt\", \"rb\") as fp:\n",
    "    hq_img_paths = pickle.load(fp)\n",
    "with open(\"datalists/idd_hq_anno_path_list.txt\", \"rb\") as fp:\n",
    "    hq_anno_paths = pickle.load(fp)\n",
    "    \n",
    "trgt_images =  hq_img_paths #non_hq_img_paths #\n",
    "trgt_annos = hq_anno_paths #non_hq_anno_paths #hq_anno_paths + \n",
    "trgt_dataset = IDD(trgt_images,trgt_annos,get_transform(train=True))\n",
    "trgt_dl =  torch.utils.data.DataLoader(trgt_dataset, batch_size=batch_size, shuffle=True, num_workers=4,collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#src_dataset[0][0].shape,trgt_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransportBlock(nn.Module):\n",
    "    def __init__(self,backbone,n_channels=256,batch_size=2):\n",
    "        super(TransportBlock, self).__init__()\n",
    "        self.backbone = backbone.cuda()\n",
    "        self.stats = [0.485, 0.456, 0.406],[0.229, 0.224, 0.225]\n",
    "        self.batch_size=2\n",
    "        self.unet = Unet(n_channels).cuda()\n",
    "        \n",
    "        for name,p in self.backbone.named_parameters():\n",
    "            p.requires_grad=False\n",
    "        \n",
    "    def unet_forward(self,x):\n",
    "        return self.unet(x)\n",
    "                \n",
    "    def transport_loss(self,S_embeddings, T_embeddings, N_cluster=5):\n",
    "        Loss = 0.  \n",
    "        for batch in range(self.batch_size):\n",
    "            S_embeddings = S_embeddings[batch].view(256,-1)\n",
    "            T_embeddings = T_embeddings[batch].view(256,-1)\n",
    "            \n",
    "            N_random_vec =  S_embeddings[np.random.choice(S_embeddings.shape[0], N_cluster)]\n",
    "\n",
    "            cluster_labels = nearest(S_embeddings, N_random_vec)\n",
    "            cluster_centroids = torch.cat([torch.mean(S_embeddings[cluster_labels == label], dim=0).unsqueeze(0) for label in cluster_labels])\n",
    "\n",
    "            Target_labels = nearest(T_embeddings, cluster_centroids)\n",
    "\n",
    "            target_centroids = []\n",
    "            for label in cluster_labels:\n",
    "                if label in Target_labels:\n",
    "                    target_centroids.append(torch.mean(T_embeddings[Target_labels == label], dim=0))\n",
    "                else:\n",
    "                    target_centroids.append(cluster_centroids[label])  \n",
    "\n",
    "            target_centroids = torch.cat(target_centroids)\n",
    "\n",
    "            dist = lambda x,y: torch.mean((x -y)**2)\n",
    "            intra_class_variance = torch.cat([dist(T_embeddings[Target_labels[label]], target_centroids[label]).unsqueeze(0) for label in cluster_labels])\n",
    "            centroid_distance = torch.cat([dist(target_centroids[label], cluster_centroids[label]).unsqueeze(0) for label in cluster_labels])\n",
    "\n",
    "            Loss += torch.mean(centroid_distance*intra_class_variance) # similar to earth mover distance\n",
    "        return Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True).cpu()\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes).cpu() # replace the pre-trained head with a new one\n",
    "    return model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load('saved_models/bdd100k_24.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model(12)\n",
    "model.load_state_dict(torch.load('saved_models/bdd100k_24.pth')['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ot = TransportBlock(model.backbone)\n",
    "params = [p for p in ot.unet.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=1e-3,momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,base_lr=1e-3,max_lr=6e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GeneralizedRCNNTransform()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from detection import transform\n",
    "transform = transform.GeneralizedRCNNTransform(min_size=800, max_size=1333, image_mean=[0.485, 0.456, 0.406], image_std=[0.229, 0.224, 0.225])\n",
    "transform.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='progress-bar-interrupted' max='1841', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      Interrupted\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mb = master_bar(range(num_epochs))\n",
    "for i in mb:\n",
    "    for trgt_img, _ in progress_bar(trgt_dl,parent=mb):\n",
    "        src_img, _ = next(iter(src_dl))\n",
    "\n",
    "        src_images = list(image.cuda() for image in src_img)\n",
    "        trgt_images = list(image.cuda() for image in trgt_img)\n",
    "\n",
    "        src_images, _ = transform(src_images, None)\n",
    "        src_features = ot.backbone(src_images.tensors)[0]\n",
    "\n",
    "        trgt_images, _ = transform(trgt_images, None)\n",
    "        trgt_features = ot.backbone(trgt_images.tensors)[0]\n",
    "        \n",
    "        torch.save(src_features,'src_features.pth')\n",
    "        torch.save(trgt_features,'trgt_features.pth')\n",
    "        \n",
    "        modified_trgt_features = ot.unet_forward(trgt_features)\n",
    "        \n",
    "        torch.save(modified_trgt_features,'modified_trgt_features.pth')\n",
    "        \n",
    "        break\n",
    "        #print(src_features.shape,modified_trgt_features.shape)\n",
    "        \n",
    "        # pad if dim of feature maps are not same\n",
    "        if src_features.shape!=modified_trgt_features.shape:\n",
    "            print(\"Earlier\", src_features.shape,modified_trgt_features.shape)\n",
    "            print(\"Fixing\")\n",
    "            if src_features.size(3)<336:\n",
    "                src_features = F.pad(src_features,(336-src_features.size(3),0,0,0)).contiguous()\n",
    "            if modified_trgt_features.size(3)>192:\n",
    "                modified_trgt_features = F.pad(modified_trgt_features,(0,0,192-modified_trgt_features.size(2),0)).contiguous()\n",
    "            if modified_trgt_features.size(3)<336:\n",
    "                modified_trgt_features = F.pad(modified_trgt_features,(336-modified_trgt_features.size(3),0,0,0)).contiguous()\n",
    "        ############################################################  \n",
    "        #print(\"Now\", src_features.shape,modified_trgt_features.shape)\n",
    "        assert src_features.shape==modified_trgt_features.shape\n",
    "\n",
    "        loss = ot.transport_loss(src_features,modified_trgt_features)\n",
    "\n",
    "        print (\"transport_loss: \",loss.item(),\"lr: \", optimizer.param_groups[0][\"lr\"])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        del src_images,trgt_images,src_features,trgt_features,_\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'model_state_dict': ot.unet.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, 'saved_models/unet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
